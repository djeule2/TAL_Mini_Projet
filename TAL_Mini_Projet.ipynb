{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "another-application",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/HEIG-VD_Logo_96x29_RVB_ROUGE.png\" alt=\"HEIG-VD Logo\" width=\"250\"/>\n",
    "\n",
    "# Cours TAL - Mini - Projet\n",
    "# Classification de dépêches d’agence avec NLTK\n",
    "\n",
    "**Objectifs**\n",
    "\n",
    "L’objectif de ce projet est de réaliser des expériences de *classification de documents* sous NLTK avec \n",
    "le corpus de dépêches Reuters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-holmes",
   "metadata": {},
   "source": [
    "*  le corpus Reuters contient environ 10'000 dépêches datant des années 1980, et il est fourni avec NLTK comme expliqué dans le [livre NLTK, ch.2](http://www.nltk.org/book/ch02.html), §1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "greek-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/kler/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "nltk.downloader.Downloader().download('reuters') \n",
    "# à exécuter une seule fois pour télécharger les fichiers localement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-gothic",
   "metadata": {},
   "source": [
    "* Le code suivant illustre certaines de ses fonctionnalités en imprimant des informations de base pour la collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "martial-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10788 documents\n",
      "7769 total train documents\n",
      "3019 total test documents\n",
      "90 categories\n",
      "['BUNDESBANK', 'ALLOCATES', '6', '.', '1', 'BILLION', ...]\n",
      "BUNDESBANK ALLOCATES 6.1 BILLION MARKS IN TENDER\n",
      "  The Bundesbank accepted bids for 6.1\n",
      "  billion marks at today's tender for a 28-day securities\n",
      "  repurchase pact at a fixed rate of 3.80 pct, a central bank\n",
      "  spokesman said.\n",
      "      Banks, which bid for a total 12.2 billion marks liquidity,\n",
      "  will be credited with the funds allocated today and must buy\n",
      "  back securities pledged on May 6.\n",
      "      Some 14.9 billion marks will drain from the market today as\n",
      "  an earlier pact expires, so the Bundesbank is effectively\n",
      "  withdrawing a net 8.1 billion marks from the market with\n",
      "  today's allocation.\n",
      "      A Bundesbank spokesman said in answer to enquiries that the\n",
      "  withdrawal of funds did not reflect a tightening of credit\n",
      "  policy, but was to be seen in the context of plentiful\n",
      "  liquidity in the banking system.\n",
      "      Banks held an average 59.3 billion marks at the Bundesbank\n",
      "  over the first six days of the month, well clear of the likely\n",
      "  April minimum reserve requirement of 51 billion marks.\n",
      "      The Bundesbank spokesman noted that by bidding only 12.2\n",
      "  billion marks, below the outgoing 14.9 billion, banks\n",
      "  themselves had shown they felt they had plenty of liquidity.\n",
      "      Dealers said the Bundesbank is keen to prevent too much\n",
      "  liquidity accruing in the market, as that would blunt the\n",
      "  effectiveness of the security repurchase agreement, its main\n",
      "  open-market instrument for steering market interest rates. Two\n",
      "  further pacts are likely this month over the next two weeks.\n",
      "      The Bundesbank is currently steering call money between 3.6\n",
      "  and 3.8 pct, although short-term fluctuations outside that\n",
      "  range are possible, dealers said.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of documents\n",
    "documents = reuters.fileids()\n",
    "print(str(len(documents)) + \" documents\");\n",
    " \n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"),\n",
    "documents));\n",
    "print(str(len(train_docs_id)) + \" total train documents\");\n",
    " \n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"),\n",
    "documents));\n",
    "print(str(len(test_docs_id)) + \" total test documents\");\n",
    " \n",
    "# List of categories\n",
    "categories = reuters.categories();\n",
    "print(str(len(categories)) + \" categories\");\n",
    " \n",
    "# Documents in a category\n",
    "category_docs = reuters.fileids(\"interest\");\n",
    " \n",
    "# Words for a document\n",
    "document_id = category_docs[0]\n",
    "document_words = reuters.words(category_docs[0]);\n",
    "print(document_words);\n",
    " \n",
    "# Raw document\n",
    "print(reuters.raw(document_id));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-bidding",
   "metadata": {},
   "source": [
    "# Hyper-paramètres \n",
    "\n",
    "Avant d’appliquer les techniques traditionnelles d’apprentissage automatique, nous devons représenter et pondérer chaque document par rapport à l’ensemble des fonctionnalités textuelles. Nous allons appliquer les transformations suivantes :\n",
    "\n",
    "* Minuscules le contenu d’origine\n",
    "* Tokeniser le texte\n",
    "* Stem chacun des jetons\n",
    "* Pondérer chacune de ces caractéristiques, pour chaque document\n",
    "* Normaliser la représentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "proprietary-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('english')\n",
    "charfilter = re.compile('[a-zA-Z]+')\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    #tokenizing the words:\n",
    "    #words = word_tokenize(text)\n",
    "    #converting all the tokens to lower case:\n",
    "    words = map(lambda word: word.lower(), text)\n",
    "    #let's remove every stopwords\n",
    "    words = [word for word in words if word not in stopWords]\n",
    "    #stemming all the tokens\n",
    "    tokens = (list(map(lambda token: PorterStemmer().stem(token), words)))\n",
    "    ntokens = list(filter(lambda token : charfilter.match(token),tokens))\n",
    "    return ntokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-natural",
   "metadata": {},
   "source": [
    "* nous pouvons créer des classifieurs qui baliseront automatiquement les nouveaux documents avec des étiquettes de catégorie appropriées. Tout d’abord, nous construisons une liste de documents, étiquetés avec les catégories appropriées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "demographic-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import random\n",
    "\n",
    "list_categorie = [\"money-fx\", \"interest\", \"money-supply\"]\n",
    "document = [(list(reuters.words(reuterid)), category)\n",
    "             for category in list_categorie\n",
    "             for reuterid in reuters.fileids(category)]\n",
    "random.shuffle(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "median-breakfast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "formed-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document): \n",
    "    document_words = set(document)\n",
    "    word_features= simple_tokenizer(document_words)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-laugh",
   "metadata": {},
   "source": [
    "# classifieurs binaires NaiveBayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "foster-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in document]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "selective-offering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "contemporary-retailer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['interest', 'money-fx', 'money-supply']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(classifier.labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fewer-convertible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           contains(feb) = False          money- : money- =     60.3 : 1.0\n",
      "           contains(mln) = False          money- : intere =     59.6 : 1.0\n",
      "            contains(m3) = False          money- : money- =     40.6 : 1.0\n",
      "           contains(plu) = False          money- : intere =     33.2 : 1.0\n",
      "         contains(defin) = False          money- : intere =     27.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "white-federal",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NaiveBayesClassifier' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-22-f5f9fc552487>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mY_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclassifier\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_set\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclassification_report\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mY_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_labels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NaiveBayesClassifier' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "Y_test = classifier.predict(test_set)\n",
    "print(classification_report(Y_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-guitar",
   "metadata": {},
   "source": [
    "# Classifieur d’arbre de décisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "historic-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import random\n",
    "\n",
    "list_categorie = [ \"grain\", \"wheat\", \"corn\"]\n",
    "document_arbre = [(list(reuters.words(reuterid)), category)\n",
    "             for category in list_categorie\n",
    "             for reuterid in reuters.fileids(category)]\n",
    "random.shuffle(document_arbre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "alike-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in document_arbre]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.classify.DecisionTreeClassifier.train(\n",
    "    train_set, entropy_cutoff=0, support_cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "tired-wheat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-assault",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classifieur multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "list_categorie = [\"money-fx\", \"grain\", \"crude\"]\n",
    "document = [(list(reuters.words(reuterid)), category)\n",
    "             for category in list_categorie\n",
    "             for reuterid in reuters.fileids(category)]\n",
    "random.shuffle(document)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in document]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores :\n",
      "crude\n",
      "Precision :\t 1.0\n",
      "Recall :\t 0.9629629629629629\n",
      "F-measure :\t 0.9811320754716982\n",
      "\n",
      "\n",
      "grain\n",
      "Precision :\t 0.9705882352941176\n",
      "Recall :\t 1.0\n",
      "F-measure :\t 0.9850746268656717\n",
      "\n",
      "\n",
      "money-fx\n",
      "Precision :\t 1.0\n",
      "Recall :\t 1.0\n",
      "F-measure :\t 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from nltk.metrics import precision, recall, f_measure\n",
    "\n",
    "\n",
    "ref_label_set = collections.defaultdict(set)\n",
    "test_label_set = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    ref_label_set[label].add(i)\n",
    "    classification_label = classifier.classify(feats)\n",
    "    test_label_set[classification_label].add(i)\n",
    "\n",
    "print('Scores :')\n",
    "\n",
    "for label in ref_label_set.keys():\n",
    "    print(label)\n",
    "    print( 'Precision :\\t', precision(ref_label_set[label], test_label_set[label]) )\n",
    "    print( 'Recall :\\t', recall(ref_label_set[label], test_label_set[label]) )\n",
    "    print( 'F-measure :\\t', f_measure(ref_label_set[label], test_label_set[label]) )\n",
    "    print('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}